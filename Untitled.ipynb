{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ea1a1b7-39ce-4d05-ac41-886ca5e86d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This file contains the module for generating\n",
    "'''\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "class QuestionExtractor:\n",
    "    ''' This class contains all the methods\n",
    "    required for extracting questions from\n",
    "    a given document\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_questions):\n",
    "\n",
    "        self.num_questions = num_questions\n",
    "\n",
    "        # hash set for fast lookup\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        # named entity recognition tagger\n",
    "        self.ner_tagger = spacy.load('en_core_web_md')\n",
    "\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "\n",
    "        self.questions_dict = dict()\n",
    "\n",
    "    def get_questions_dict(self, document):\n",
    "        '''\n",
    "        Returns a dict of questions in the format:\n",
    "        question_number: {\n",
    "            question: str\n",
    "            answer: str\n",
    "        }\n",
    "\n",
    "        Params:\n",
    "            * document : string\n",
    "        Returns:\n",
    "            * dict\n",
    "        '''\n",
    "        # find candidate keywords\n",
    "        self.candidate_keywords = self.get_candidate_entities(document)\n",
    "\n",
    "        # set word scores before ranking candidate keywords\n",
    "        self.set_tfidf_scores(document)\n",
    "\n",
    "        # rank the keywords using calculated tf idf scores\n",
    "        self.rank_keywords()\n",
    "\n",
    "        # form the questions\n",
    "        self.form_questions()\n",
    "\n",
    "        return self.questions_dict\n",
    "\n",
    "    def get_filtered_sentences(self, document):\n",
    "        ''' Returns a list of sentences - each of\n",
    "        which has been cleaned of stopwords.\n",
    "        Params:\n",
    "                * document: a paragraph of sentences\n",
    "        Returns:\n",
    "                * list<str> : list of string\n",
    "        '''\n",
    "        sentences = sent_tokenize(document)  # split documents into sentences\n",
    "\n",
    "        return [self.filter_sentence(sentence) for sentence in sentences]\n",
    "\n",
    "    def filter_sentence(self, sentence):\n",
    "        '''Returns the sentence without stopwords\n",
    "        Params:\n",
    "                * sentence: A string\n",
    "        Returns:\n",
    "                * string\n",
    "        '''\n",
    "        words = word_tokenize(sentence)\n",
    "        return ' '.join(w for w in words if w not in self.stop_words)\n",
    "\n",
    "    def get_candidate_entities(self, document):\n",
    "        ''' Returns a list of entities according to\n",
    "        spacy's ner tagger. These entities are candidates\n",
    "        for the questions\n",
    "\n",
    "        Params:\n",
    "                * document : string\n",
    "        Returns:\n",
    "                * list<str>\n",
    "        '''\n",
    "        entities = self.ner_tagger(document)\n",
    "        entity_list = []\n",
    "\n",
    "        for ent in entities.ents:\n",
    "            entity_list.append(ent.text)\n",
    "\n",
    "        return list(set(entity_list))  # remove duplicates\n",
    "\n",
    "    def set_tfidf_scores(self, document):\n",
    "        ''' Sets the tf-idf scores for each word'''\n",
    "        self.unfiltered_sentences = sent_tokenize(document)\n",
    "        self.filtered_sentences = self.get_filtered_sentences(document)\n",
    "\n",
    "        self.word_score = dict()  # (word, score)\n",
    "\n",
    "        # (word, sentence where word score is max)\n",
    "        self.sentence_for_max_word_score = dict()\n",
    "\n",
    "        tf_idf_vector = self.vectorizer.fit_transform(self.filtered_sentences)\n",
    "        feature_names = self.vectorizer.get_feature_names()\n",
    "        tf_idf_matrix = tf_idf_vector.todense().tolist()\n",
    "\n",
    "        num_sentences = len(self.unfiltered_sentences)\n",
    "        num_features = len(feature_names)\n",
    "\n",
    "        for i in range(num_features):\n",
    "            word = feature_names[i]\n",
    "            self.sentence_for_max_word_score[word] = \"\"\n",
    "            tot = 0.0\n",
    "            cur_max = 0.0\n",
    "\n",
    "            for j in range(num_sentences):\n",
    "                tot += tf_idf_matrix[j][i]\n",
    "\n",
    "                if tf_idf_matrix[j][i] > cur_max:\n",
    "                    cur_max = tf_idf_matrix[j][i]\n",
    "                    self.sentence_for_max_word_score[word] = self.unfiltered_sentences[j]\n",
    "\n",
    "            # average score for each word\n",
    "            self.word_score[word] = tot / num_sentences\n",
    "\n",
    "    def get_keyword_score(self, keyword):\n",
    "        ''' Returns the score for a keyword\n",
    "        Params:\n",
    "            * keyword : string of possible several words\n",
    "        Returns:\n",
    "            * float : score\n",
    "        '''\n",
    "        score = 0.0\n",
    "        for word in word_tokenize(keyword):\n",
    "            if word in self.word_score:\n",
    "                score += self.word_score[word]\n",
    "        return score\n",
    "\n",
    "    def get_corresponding_sentence_for_keyword(self, keyword):\n",
    "        ''' Finds and returns a sentence containing\n",
    "        the keywords\n",
    "        '''\n",
    "        words = word_tokenize(keyword)\n",
    "        for word in words:\n",
    "\n",
    "            if word not in self.sentence_for_max_word_score:\n",
    "                continue\n",
    "\n",
    "            sentence = self.sentence_for_max_word_score[word]\n",
    "\n",
    "            all_present = True\n",
    "            for w in words:\n",
    "                if w not in sentence:\n",
    "                    all_present = False\n",
    "\n",
    "            if all_present:\n",
    "                return sentence\n",
    "        return \"\"\n",
    "\n",
    "    def rank_keywords(self):\n",
    "        '''Rank keywords according to their score'''\n",
    "        self.candidate_triples = []  # (score, keyword, corresponding sentence)\n",
    "\n",
    "        for candidate_keyword in self.candidate_keywords:\n",
    "            self.candidate_triples.append([\n",
    "                self.get_keyword_score(candidate_keyword),\n",
    "                candidate_keyword,\n",
    "                self.get_corresponding_sentence_for_keyword(candidate_keyword)\n",
    "            ])\n",
    "\n",
    "        self.candidate_triples.sort(reverse=True)\n",
    "\n",
    "    def form_questions(self):\n",
    "        ''' Forms the question and populates\n",
    "        the question dict\n",
    "        '''\n",
    "        used_sentences = list()\n",
    "        idx = 0\n",
    "        cntr = 1\n",
    "        num_candidates = len(self.candidate_triples)\n",
    "        while cntr <= self.num_questions and idx < num_candidates:\n",
    "            candidate_triple = self.candidate_triples[idx]\n",
    "\n",
    "            if candidate_triple[2] not in used_sentences:\n",
    "                used_sentences.append(candidate_triple[2])\n",
    "\n",
    "                self.questions_dict[cntr] = {\n",
    "                    \"question\": candidate_triple[2].replace(\n",
    "                        candidate_triple[1],\n",
    "                        '_' * len(candidate_triple[1])),\n",
    "                    \"answer\": candidate_triple[1]\n",
    "                }\n",
    "\n",
    "                cntr += 1\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f0f0760-081c-45e0-8143-699f8b42848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "class QuestionGeneration:\n",
    "    '''This class contains the method\n",
    "    to generate questions\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_questions, num_options):\n",
    "        self.num_questions = num_questions\n",
    "        self.num_options = num_options\n",
    "        # Initialize instances of QuestionExtractor and IncorrectAnswerGenerator\n",
    "        self.question_extractor = QuestionExtractor(num_questions)\n",
    "        self.incorrect_answer_generator = IncorrectAnswerGenerator()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = text.replace('\\n', ' ')  # remove newline chars\n",
    "        sentences = sent_tokenize(text)\n",
    "        cleaned_text = \"\"\n",
    "        for sentence in sentences:\n",
    "            # remove non alphanumeric chars\n",
    "            cleaned_sentence = re.sub(r'([^\\s\\w]|_)+', '', sentence)\n",
    "            # substitute multiple spaces with single space\n",
    "            cleaned_sentence = re.sub(' +', ' ', cleaned_sentence)\n",
    "            cleaned_text += cleaned_sentence\n",
    "\n",
    "            if cleaned_text[-1] == ' ':\n",
    "                cleaned_text = cleaned_text[:-1] + '.'\n",
    "            else:\n",
    "                cleaned_text += '.'\n",
    "\n",
    "            cleaned_text += ' '  # pad with space at end\n",
    "        return cleaned_text\n",
    "\n",
    "    def generate_questions_dict(self, document):\n",
    "        document = self.clean_text(document)\n",
    "        self.questions_dict = self.question_extractor.get_questions_dict(document)\n",
    "\n",
    "        for i in range(1, self.num_questions + 1):\n",
    "            if i not in self.questions_dict:\n",
    "                continue\n",
    "            self.questions_dict[i][\"options\"] = self.incorrect_answer_generator.get_all_options_dict(\n",
    "                self.questions_dict[i][\"answer\"], self.num_options\n",
    "            )\n",
    "\n",
    "        return self.questions_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b00021d1-d7d0-4455-bc82-bc28bb2d4f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncorrectAnswerGenerator:\n",
    "    ''' This class contains the methods\n",
    "    for generating the incorrect answers\n",
    "    given an answer\n",
    "    '''\n",
    "\n",
    "    def __init__(self, document):\n",
    "        # model required to fetch similar words\n",
    "        self.model = api.load(\"glove-wiki-gigaword-100\")\n",
    "        self.all_words = []\n",
    "        for sent in sent_tokenize(document):\n",
    "            self.all_words.extend(word_tokenize(sent))\n",
    "        self.all_words = list(set(self.all_words))\n",
    "\n",
    "    def get_all_options_dict(self, answer, num_options):\n",
    "        ''' This method returns a dict\n",
    "        of 'num_options' options out of\n",
    "        which one is correct and is the answer\n",
    "        '''\n",
    "        options_dict = dict()\n",
    "        try:\n",
    "            similar_words = self.model.similar_by_word(answer, topn=15)[::-1]\n",
    "\n",
    "            for i in range(1, num_options + 1):\n",
    "                options_dict[i] = similar_words[i - 1][0]\n",
    "\n",
    "        except:\n",
    "            self.all_sim = []\n",
    "            for word in self.all_words:\n",
    "                if word not in answer:\n",
    "                    try:\n",
    "                        self.all_sim.append(\n",
    "                            (self.model.similarity(answer, word), word))\n",
    "                    except:\n",
    "                        self.all_sim.append(\n",
    "                            (0.0, word))\n",
    "                else:\n",
    "                    self.all_sim.append((-1.0, word))\n",
    "\n",
    "            self.all_sim.sort(reverse=True)\n",
    "\n",
    "            for i in range(1, num_options+1):\n",
    "                options_dict[i] = self.all_sim[i-1][1]\n",
    "\n",
    "        replacement_idx = random.randint(1, num_options)\n",
    "\n",
    "        options_dict[replacement_idx] = answer\n",
    "\n",
    "        return options_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d40ea79-0f0d-4d97-8b59-d0fd605a6830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Using cached pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af828cf5-cfe5-46cf-a111-9aeabf774e95",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'question_generation_main'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPyPDF2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PdfFileReader\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mquestion_generation_main\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QuestionGeneration\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpdf2text\u001b[39m(file_path: \u001b[38;5;28mstr\u001b[39m, file_exten: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Converts a given file to text content \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'question_generation_main'"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfFileReader\n",
    "\n",
    "from question_generation_main import QuestionGeneration\n",
    "\n",
    "\n",
    "def pdf2text(file_path: str, file_exten: str) -> str:\n",
    "    \"\"\" Converts a given file to text content \"\"\"\n",
    "\n",
    "    _content = ''\n",
    "\n",
    "    # Identify file type and get its contents\n",
    "    if file_exten == 'pdf':\n",
    "        with open(file_path, 'rb') as pdf_file:\n",
    "            _pdf_reader = PdfFileReader(pdf_file)\n",
    "            for p in range(_pdf_reader.numPages):\n",
    "                _content += _pdf_reader.getPage(p).extractText()\n",
    "            # _content = _pdf_reader.getPage(0).extractText()\n",
    "            print('PDF operation done!')\n",
    "\n",
    "    elif file_exten == 'txt':\n",
    "        with open(file_path, 'r') as txt_file:\n",
    "            _content = txt_file.read()\n",
    "            print('TXT operation done!')\n",
    "\n",
    "    return _content\n",
    "\n",
    "\n",
    "def txt2questions(doc: str, n=5, o=4) -> dict:\n",
    "    \"\"\" Get all questions and options \"\"\"\n",
    "\n",
    "    qGen = QuestionGeneration(n, o)\n",
    "    q = qGen.generate_questions_dict(doc)\n",
    "    for i in range(len(q)):\n",
    "        temp = []\n",
    "        for j in range(len(q[i + 1]['options'])):\n",
    "            temp.append(q[i + 1]['options'][j + 1])\n",
    "        # print(temp)\n",
    "        q[i + 1]['options'] = temp\n",
    "    return q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b3c4989-9ede-4beb-915c-7c3dd7499960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement question_generation_main (from versions: none)\n",
      "ERROR: No matching distribution found for question_generation_main\n"
     ]
    }
   ],
   "source": [
    "!pip install question_generation_main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9852554-5e09-4d8a-b39c-a34ebc1ede20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
