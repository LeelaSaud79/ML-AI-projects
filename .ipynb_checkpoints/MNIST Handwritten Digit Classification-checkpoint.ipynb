{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5f4c6fe-b10b-4224-9221-c03451adbc4f",
   "metadata": {},
   "source": [
    "!pip install bert-extractive-summarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "243c7653-babb-4230-9e3b-255ffa789a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1- Import the text file/article that has to be used for MCQ generation\n",
    "\n",
    "file = open(\"article.txt\", \"r\", encoding=\"utf-8\")\n",
    "text = file.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f25b680-8889-4785-8a69-3c074b1f0168",
   "metadata": {},
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc40512-5971-4e3f-8cfa-8a5209faf404",
   "metadata": {},
   "source": [
    "!pip install git+https://github.com/boudinfl/pke.git\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b83a6c6d-2ada-4ee5-a4bf-6517875c311a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0ebb132-dc16-449a-9dd3-47c9e2747080",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "No downloaded spacy model for 'en' language.\nA list of downloadable spacy models is available at https://spacy.io/models.\nAlternatively, preprocess your document as a list of sentence tuple (word, pos), such as:\n\t[[('The', 'DET'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('.', 'PUNCT')]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m         result\u001b[38;5;241m.\u001b[39mappend(each[\u001b[38;5;241m0\u001b[39m]) \n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m---> 20\u001b[0m impWords\u001b[38;5;241m=\u001b[39m\u001b[43mgetImportantWords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#Get the important words (keywords) from the text article using the above function\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#print(impWords)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 8\u001b[0m, in \u001b[0;36mgetImportantWords\u001b[1;34m(art)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetImportantWords\u001b[39m(art): \n\u001b[0;32m      7\u001b[0m     extractor\u001b[38;5;241m=\u001b[39mpke\u001b[38;5;241m.\u001b[39munsupervised\u001b[38;5;241m.\u001b[39mMultipartiteRank() \u001b[38;5;66;03m#Using the Multipartite Unsupervised Extractor as our extractor\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_document\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mart\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     pos\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPROPN\u001b[39m\u001b[38;5;124m'\u001b[39m} \u001b[38;5;66;03m#We are only considering proper nouns as valid candidates for our keywords\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     stops\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(string\u001b[38;5;241m.\u001b[39mpunctuation) \u001b[38;5;66;03m#Stoplist contains the words to be avoided\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pke\\base.py:98\u001b[0m, in \u001b[0;36mLoadFile.load_document\u001b[1;34m(self, input, language, stoplist, normalization, spacy_model)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     97\u001b[0m     parser \u001b[38;5;241m=\u001b[39m RawTextReader(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage)\n\u001b[1;32m---> 98\u001b[0m     sents \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspacy_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspacy_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# check whether input is processed text\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pke\\readers.py:93\u001b[0m, in \u001b[0;36mRawTextReader.read\u001b[1;34m(self, text, spacy_model)\u001b[0m\n\u001b[0;32m     91\u001b[0m     excp_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAlternatively, preprocess your document as a list of sentence tuple (word, pos), such as:\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m     excp_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m[[(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDET\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrown\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mADJ\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfox\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNOUN\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m), (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPUNCT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)]]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(excp_msg)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# add the sentence splitter\u001b[39;00m\n\u001b[0;32m     96\u001b[0m nlp\u001b[38;5;241m.\u001b[39madd_pipe(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentencizer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: No downloaded spacy model for 'en' language.\nA list of downloadable spacy models is available at https://spacy.io/models.\nAlternatively, preprocess your document as a list of sentence tuple (word, pos), such as:\n\t[[('The', 'DET'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('.', 'PUNCT')]]"
     ]
    }
   ],
   "source": [
    "#Step 2- Extract the important words(keywords) from the text article that can be used to create MCQ using PKE (Python Keyword Extraction)\n",
    "\n",
    "import pke\n",
    "from nltk.corpus import stopwords #Stopwords are the words that we need to avoid while considering keyword extraction\n",
    "import string\n",
    "def getImportantWords(art): \n",
    "    extractor=pke.unsupervised.MultipartiteRank() #Using the Multipartite Unsupervised Extractor as our extractor\n",
    "    extractor.load_document(input=art,language='en')\n",
    "    pos={'PROPN'} #We are only considering proper nouns as valid candidates for our keywords\n",
    "    stops=list(string.punctuation) #Stoplist contains the words to be avoided\n",
    "    stops+=['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-'] #These stand for the brackets as in lrb=left round bracket=\"(\" and so on\n",
    "    stops+=stopwords.words('english')\n",
    "    extractor.candidate_selection(pos=pos,stoplist=stops) #Sets the candidate selection criteria, as in, which should be considered and which should be avoided\n",
    "    extractor.candidate_weighting() #Sets the preference criteria for the candidates\n",
    "    result=[] \n",
    "    ex=extractor.get_n_best(n=25) #Gets the 25 best candidates according to the criteria set\n",
    "    for each in ex:\n",
    "        result.append(each[0]) \n",
    "    return result\n",
    "impWords=getImportantWords(text) #Get the important words (keywords) from the text article using the above function\n",
    "#print(impWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "066cfafc-b4bf-4e30-9912-cca9a3fa5a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3- Split the whole text article into an array/list of individual sentences. This will help us fetch the sentences related to the keywords easily\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "def splitTextToSents(art):\n",
    "    s=[sent_tokenize(art)]\n",
    "    s=[y for x in s for y in x]\n",
    "    s=[sent.strip() for sent in s if len(sent)>15] #Removes all the sentences that have length less than 15 so that we can ensure that our questions have enough length for context\n",
    "    return s\n",
    "sents=splitTextToSents(text) #Achieve a well splitted set of sentences from the text article\n",
    "#print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "758df006-aa6d-4ce6-a697-c9d7b669cb5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flashtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Step 4- Map the sentences which contain the keywords to the related keywords so that we can easily lookup the sentences related to the keywords\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflashtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KeywordProcessor\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmapSents\u001b[39m(impWords,sents):\n\u001b[0;32m      5\u001b[0m     processor\u001b[38;5;241m=\u001b[39mKeywordProcessor() \u001b[38;5;66;03m#Using keyword processor as our processor for this task\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'flashtext'"
     ]
    }
   ],
   "source": [
    "#Step 4- Map the sentences which contain the keywords to the related keywords so that we can easily lookup the sentences related to the keywords\n",
    "\n",
    "from flashtext import KeywordProcessor\n",
    "def mapSents(impWords,sents):\n",
    "    processor=KeywordProcessor() #Using keyword processor as our processor for this task\n",
    "    keySents={}\n",
    "    for word in impWords:\n",
    "        keySents[word]=[]\n",
    "        processor.add_keyword(word) #Adds key word to the processor\n",
    "    for sent in sents:\n",
    "        found=processor.extract_keywords(sent) #Extract the keywords in the sentence\n",
    "        for each in found:\n",
    "            keySents[each].append(sent) #For each keyword found, map the sentence to the keyword\n",
    "    for key in keySents.keys():\n",
    "        temp=keySents[key]\n",
    "        temp=sorted(temp,key=len,reverse=True) #Sort the sentences according to their decreasing length in order to ensure the quality of question for the MCQ \n",
    "        keySents[key]=temp\n",
    "    return keySents\n",
    "mappedSents=mapSents(impWords,sents) #Achieve the sentences that contain the keywords and map those sentences to the keywords using this function\n",
    "#print(mappedSents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c41fa65b-66be-4178-80c3-506356ab5761",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pywsd'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Step 5- Get the sense of the word. In order to attain a quality set of distractors we need to get the right sense of the keyword. This is explained in detail in the seperate alogrithm documentation\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpywsd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimilarity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m max_similarity\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpywsd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlesk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adapted_lesk\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpywsd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlesk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m simple_lesk\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pywsd'"
     ]
    }
   ],
   "source": [
    "#Step 5- Get the sense of the word. In order to attain a quality set of distractors we need to get the right sense of the keyword. This is explained in detail in the seperate alogrithm documentation\n",
    "\n",
    "from pywsd.similarity import max_similarity\n",
    "from pywsd.lesk import adapted_lesk\n",
    "from pywsd.lesk import simple_lesk\n",
    "from pywsd.lesk import cosine_lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "def getWordSense(sent,word):\n",
    "    word=word.lower() \n",
    "    if len(word.split())>0: #Splits the word with underscores(_) instead of spaces if there are multiple words\n",
    "        word=word.replace(\" \",\"_\")\n",
    "    synsets=wn.synsets(word,'n') #Sysnets from Google are invoked\n",
    "    if synsets:\n",
    "        wup=max_similarity(sent,word,'wup',pos='n')\n",
    "        adapted_lesk_output = adapted_lesk(sent, word, pos='n')\n",
    "        lowest_index=min(synsets.index(wup),synsets.index(adapted_lesk_output))\n",
    "        return synsets[lowest_index]\n",
    "    else:\n",
    "        return None\n",
    "#print(\"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed30005b-884d-4a0d-b37c-8f6c8c82010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6- Get distractor from WordNet. These distractors work on the basis of hypernym and hyponym explained in detail in the documentation.\n",
    "\n",
    "def getDistractors(syn,word):\n",
    "    dists=[]\n",
    "    word=word.lower()\n",
    "    actword=word\n",
    "    if len(word.split())>0: #Splits the word with underscores(_) instead of spaces if there are multiple words\n",
    "        word.replace(\" \",\"_\")\n",
    "    hypernym = syn.hypernyms() #Gets the hypernyms of the word\n",
    "    if len(hypernym)==0: #If there are no hypernyms for the current word, we simple return the empty list of distractors\n",
    "        return dists\n",
    "    for each in hypernym[0].hyponyms(): #Other wise we find the relevant hyponyms for the hypernyms\n",
    "        name=each.lemmas()[0].name()\n",
    "        if(name==actword):\n",
    "            continue\n",
    "        name=name.replace(\"_\",\" \")\n",
    "        name=\" \".join(w.capitalize() for w in name.split())\n",
    "        if name is not None and name not in dists: #If the word is not already present in the list and is different from he actial word\n",
    "            dists.append(name)\n",
    "    return dists\n",
    "#print(\"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9355261-51ef-4850-ab3a-b270daa02de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7- The primary goal of this step is to take our MCQ quality one step further. The WordNet might some times fail to produce a hypernym for some words.\n",
    "#In that case the ConcepNet comes into play as they help achieve our distractors when there are no hypernyms present for it in the WordNet. More about this is discussed\n",
    "#in the algorithm documentation.\n",
    "\n",
    "import requests\n",
    "import json\n",
    "def getDistractors2(word):\n",
    "    word=word.lower()\n",
    "    actword=word\n",
    "    if len(word.split())>0: #Splits the word with underscores(_) instead of spaces if there are multiple words\n",
    "        word=word.replace(\" \",\"_\")\n",
    "    dists=[]\n",
    "    url= \"http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/PartOf&start=/c/en/%s&limit=5\"%(word,word) #To get ditractors from ConceptNet's API\n",
    "    obj=requests.get(url).json()\n",
    "    for edge in obj['edges']:\n",
    "        link=edge['end']['term']\n",
    "        url2=\"http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10\"%(link,link)\n",
    "        obj2=requests.get(url2).json()\n",
    "        for edge in obj2['edges']:\n",
    "            word2=edge['start']['label']\n",
    "            if word2 not in dists and actword.lower() not in word2.lower(): #If the word is not already present in the list and is different from he actial word\n",
    "                dists.append(word2)\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7243786a-ba05-496a-aa0a-b27df3490c5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mappedSents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Step 8- Find and map the distractors to the keywords\u001b[39;00m\n\u001b[0;32m      3\u001b[0m mappedDists\u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m each \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmappedSents\u001b[49m:\n\u001b[0;32m      5\u001b[0m     wordsense\u001b[38;5;241m=\u001b[39mgetWordSense(mappedSents[each][\u001b[38;5;241m0\u001b[39m],each) \u001b[38;5;66;03m#gets the sense of the word\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wordsense: \u001b[38;5;66;03m#if the wordsense is not null/none\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mappedSents' is not defined"
     ]
    }
   ],
   "source": [
    "#Step 8- Find and map the distractors to the keywords\n",
    "\n",
    "mappedDists={}\n",
    "for each in mappedSents:\n",
    "    wordsense=getWordSense(mappedSents[each][0],each) #gets the sense of the word\n",
    "    if wordsense: #if the wordsense is not null/none\n",
    "        dists=getDistractors(wordsense,each) #Gets the WordNet distractors\n",
    "        if len(dists)==0: #If there are no WordNet distractors available for the current word\n",
    "            dists=getDistractors2(each) #The gets the distractors from the ConceptNet API\n",
    "        if len(dists)!=0: #If there are indeed distractors from WordNet available, then maps them\n",
    "            mappedDists[each]=dists\n",
    "    else: #If there is no wordsense, the directly searches/uses the ConceptNet\n",
    "        dists=getDistractors2(each)\n",
    "        if len(dists)>0: #If it gets the Distractors then maps them\n",
    "            mappedDists[each]=dists\n",
    "#print(mappedDists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f1ebba7-9ea9-4781-a0c4-71e8a43e7515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************        Multiple Choice Questions        *******************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Step 9- The final step is to present our MCQ in a nice and readable manner.\n",
    "\n",
    "print(\"**************************************        Multiple Choice Questions        *******************************\")\n",
    "print()\n",
    "import re\n",
    "import random\n",
    "iterator = 1 #To keep the count of the questions\n",
    "for each in mappedDists:\n",
    "    sent=mappedSents[each][0]\n",
    "    p=re.compile(each,re.IGNORECASE) #Converts into regular expression for pattern matching\n",
    "    op=p.sub(\"________\",sent) #Replaces the keyword with underscores(blanks)\n",
    "    print(\"Question %s-> \"%(iterator),op) #Prints the question along with a question number\n",
    "    options=[each.capitalize()]+mappedDists[each] #Capitalizes the options\n",
    "    options=options[:4] #Selects only 4 options\n",
    "    opts=['a','b','c','d']\n",
    "    random.shuffle(options) #Shuffle the options so that order is not always same\n",
    "    for i,ch in enumerate(options):\n",
    "        print(\"\\t\",opts[i],\") \", ch) #Print the options\n",
    "    print()\n",
    "    iterator+=1 #Increase the counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5b371e-7ab1-44c0-80d6-a1397321767c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da478684-0754-4700-9ffe-6c7e37a0d4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
